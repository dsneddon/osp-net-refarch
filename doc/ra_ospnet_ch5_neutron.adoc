[chapter 5]
== Implementing OpenStack Networking (Neutron)

=== Configure Neutron Networks

Before launching any virtual machines, it is necessary to configure the virtual infrastructure, including networks that the VMs use. This section describes the configuration of Neutron networks for the OpenStack deployment.

==== Create Tenant Networks and Subnets

Tenant networks can be created using Horizon. If no gateway is specified, the first IP address on the network is used as the gateway.

Here are the networks that were created for this reference architecture:

.Reference Architecture Networks
[options="header"]
|====
|Network Name|Network Subnet|Gateway
|default-net|172.21.0.0/24|172.21.0.1
|ext-net|10.8.148.0/24|10.8.148.254
|====

External networks must be created to enable access outside the cloud. This is usually done on the command line of the Undercloud using the Overcloud credentials (run "source overcloudrc"). The Neutron router on the external network is connected to both the Public network and the External network(s), and performs many-to-one source NAT for VMs without Floating IPs as well as 1:1 NAT for VMs with Floating IPs. Horizon does not support creating external networks, so they can be created on the command-line with this command (assuming the floating IPs are on the bridge mapped to "datacentre"):

[subs=+quotes]
----
 $ *neutron net-create ext-net --router:external \
 --provider:physical_network datacentre \
 --provider:network_type vlan \
 --provider:segmentation_id 104*
----

NOTE: It is recommended that the Floating IP networks be implemented
as VLANs as in the above command, because this allows for multiple
Floating IP networks and/or multiple Provider External networks. If,
however, the native VLAN is used on a link to provide floating IPs,
then the following command creates the floating IP network directly on the bridge with no VLAN ID:

[subs=+quotes]
----
 $ *neutron net-create ext-net --router:external \
 --provider:physical_network datacentre \
 --provider:network_type flat*
----

Next a subnet must be created for the network, and a range of IPs
for Floating IPs are  assigned:

[subs=+quotes]
----
 $ *neutron subnet-create --name ext-subnet \
 --enable_dhcp=False \
 --allocation-pool start=10.8.148.50,end=10.8.148.100 \
 --gateway 10.8.148.254 \
 ext-net 10.8.148.0/24*
----

A subnet can be added to the network in Horizon by selecting *More* >
*Add Subnet* for the network in the *Networks* tab. The external
gateway must be specified. In the *Subnet Detail*, *Enable DHCP* must be disabled for External networks.

Next we create a default network for tenant VMs. This network is
internal to the Overcloud, and external access is through a floating IP.

[subs=+quotes]
----
 $ *neutron net-create default-net –provider:vxlan*
 $ *neutron subnet-create --name default-subnet default-net
 172.21.0.0/22*
----

Nova instances can then be created and attached to the default-net network. Optionally, Floating IP addresses may be associated with the instance for external connectivity.

==== Gateways and L3 Agents

Neutron routers perform the task of forwarding traffic between
networks. A router can be attached to multiple tenant networks, and
routes traffic between them. A router can also be attached to one or
more external networks, permitting outbound SNAT for VMs, or allowing
floating IPs on the external network to be assigned to VMs on the
tenant networks that the router is attached to. When interfaces are
created on the router, ports are attached to the _neutron-l3-agent_ process on the Network Controller(s).

When a router is created in Neutron, initially it is not connected to any networks. The router needs to be attached to the networks by creating interfaces, either in Horizon or on the command line.

Steps for creating routers to route traffic between Neutron networks:

1. Create the router in Horizon, giving the router a name in the dialog box, or run the following command:
+
[subs=+quotes]
----
 # *neutron router-create router-1to2*
 Created a new router:
 +-----------------------+--------------------------------------+
 | Field                 | Value                                |
 +-----------------------+--------------------------------------+
 | admin_state_up        | True                                 |
 | external_gateway_info |                                      |
 | id                    | 63285650-16b9-46e5-9a55-34701c972605 |
 | name                  | router-1to2                          |
 | status                | ACTIVE                               |
 | tenant_id             | eda22eb1991d4af285cbf60bb6847b84     |
 +-----------------------+--------------------------------------+
----
+
2. Once the router is created, attach interfaces to the router in Horizon, or run the following command, using the name of the router and the name or ID of the subnet to attach to the router.
+
[subs=+quotes]
----
# *neutron router-interface-add router-1to2 subnet=subnet-1*
Added interface 7fa87edf-6504-4d50-9886-98951baa9c93 to router router-1to2.
# *neutron router-interface-add router-1to2 subnet=subnet-2*
Added interface fbe69fb7-d5b2-4ea6-835a-9ae8b6274853 to router router-1to2.
----

The above commands create a router named “router-1to2” between the subnets named “subnet-1” and “subnet-2” using the default router address (the first IP address on the subnet) on each subnet.

To use a different IP (other than the first IP in the network), first
create a port with the desired IP using the _neutron port-create_
command, then use the parameter port= instead of subnet= with the
_neutron router-interface-add_ command.

==== Configure MTU

The Ethernet Maximum Transmission Unit (MTU) specifies the maximum
amount of data that can be transmitted in a single Ethernet frame.
Configuring the correct MTU everywhere can optimize performance,
particularly when using high speed Ethernet interfaces (10Gb or
above). Even when lower speed links are used, such as 1Gb Ethernet, a
high MTU reduces overhead, and a consistent MTU reduces fragmentation. Special care must be taken with MTU when tunneling traffic with GRE or VXLAN, since both of those technologies require extra bytes for the tunneling headers.

Typically a modern Ethernet switch that supports “Jumbo” frames
supports an MTU of at least 9000 bytes, plus 4 for the VLAN identifier. Some switches support higher MTU, but it depends on make and model.

NOTE: The MTU used on the VMs, plus any overhead for tunneling in GRE or VXLAN mode, must not exceed the MTU set on the physical interface or bond. If the Tenant tunneling network interface is set for an MTU of 9000, then the MTU on the VM cannot exceed 9000 in VLAN mode, or 8950 in GRE or VXLAN mode, in order to make room for up to 50 bytes of overhead. For more information about setting the physical MTU, refer to the section in Chapter 4 titled "Configuring Jumbo Frames".

In order to use jumbo frames on VMs, a setting must be edited or
created in two places on each OpenStack Networking controller and on
each compute host. The changes to the MTU settings are persistent
because Puppet does not manage the MTU settings in {ro} 7. The VIF attached to each VM must have an MTU that is 50 bytes lower to accommodate VXLAN or GRE overhead.

In _/etc/nova/nova.conf_, edit or create this line:

[subs=+quotes]
----
 network_device_mtu=8950
----

In _/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini_:

[subs=+quotes]
----
 veth_mtu = 8950
----

NOTE:  When the MTU is modified in Nova and Neutron, the services must be restarted for the settings to take effect. The settings only apply to VMs started after the services have been restarted.

Furthermore, the MTU on the VM image should be set to the same value, which can be done manually with the following command:

[subs=+quotes]
----
 # *ifconfig <interface> mtu 8950 up*
----

To enforce that each VM is set to use jumbo frames each time it
boots, the _dhcp-option-force_ option must be set for the
_neutron-dhcp-agent_. Change the DHCP options in the file
_/etc/neutron/dnsmasq-neutron.conf_ on all controllers:

[subs=+quotes]
----
 dhcp-option-force=26,8950
----

After making this change, restart the _neutron-dhcp-agent_ process on all controllers.

==== Configuring Provider Networks
Provider networks are networks that are attached directly to compute hosts, but traffic is not routed through the Neutron controller. Instead, provider networks are a way to attach a VM directly to a flat or VLAN network that is a part of the datacenter network. This is often how external access is provided, rather than using floating IPs on a Neutron controller. Provider networks and Neutron tenant networks can both be used simultaneously.

Provider networks can only be configured by an administrator, but they appear as normal Neutron networks to the client. This allows them to be selected by a tenant when launching a VM.
Provider networks do not require the use of the L3 Agent, because the traffic is not routed through the Neutron controller. Generally Neutron will provide DHCP services on provider networks. Ordinarily, metadata services are provided by a redirect on the L3 Agent, but another mechanism is provided below for metadata services on provider networks.

===== Configuring Neutron For Provider Networks

Neutron maps provider networks to a bridge, and maps that bridge to a physical adapter or VLAN interface. These mappings must be made in the _ovs_neutron_plugin.ini_ file on the controllers and compute hosts:

[subs=+quotes]
----
 #/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini
 bridge_mappings = physnet-trunk:br-trunk
 network_vlan_ranges = physnet-trunk
----

===== Configuring the Metadata Service For Provider Networks

The normal mechanism of providing metadata services via a redirect on the L3 Agent is not compatible with provider networks. Instead, configure the file _/etc/neutron/dhcp_agent.ini_ as follows:

[subs=+quotes]
----
 #/etc/neutron/dhcp_agent.ini
 enable_isolated_metadata = True
 enable_metadata_network = True
----

NOTE: Utilizing this configuration will supersede the L3-provided metadata services for networks that do not use provider networks, but this method should also be compatible with those networks.

===== Creating Provider Network Bridge With OSP-Director

In order to use provider networks, the interface to the network will have to be placed on a bridge. By default, a br-ex bridge will be created, but provider networks can also be assigned to interfaces which are not part of br-ex by creating another bridge. The bridge should match on both the controller and compute nodes. For example, if bond2 (with nic5 and nic6) will be attached to provider networks, then adding this to both the controller and compute NIC configuration will allow provider networks to be created on this bond:

[subs=+quotes]
----
            -
              type: ovs_bridge
              name: br-trunk
              members:
                -
                  type: ovs_bond
                  name: bond2
                  ovs_options: {get_param: BondInterfaceOvsOptions}
                  members:
                    -
                      type: interface
                      name: nic5
                      primary: true
                    -
                      type: interface
                      name: nic6
----

===== Creating Provider Network Bridge Manually

If the bridges were not created at deployment time, then create the
bridges that were referenced in the _ovs_neutron_plugin.ini_ file on the controllers and compute hosts.
To add a VLAN interface that is trunked to the host via bond2, add the bond2 interface:

[subs=+quotes]
----
 # ovs-vsctl add-br br-trunk
 # ovs-vsctl add-port br-trunk bond2
----

Alternately, to add a physical interface eth3 with a flat network:

[subs=+quotes]
----
 # ovs-vsctl add-br br-trunk
 # ovs-vsctl add-port br-trunk eth3
----

At this point the Neutron services will have to be restarted on all controllers and compute hosts. If making the changes on an HA deployment, restart only one controller at a time and wait for it to rejoin the cluster before restarting the services on the next server.

===== Validating Bridge Mapping

Neutron should be aware of all the bridge mappings on all compute hosts. To validate this, use the Neutron commands to show each compute host:

[subs=+quotes]
----
 # *neutron agent-list*
 # *neutron agent-show <uuid>*
----

You should see this in the data returned by the _agent-show_ command:

[subs=+quotes]
----
    "bridge_mappings": {
                "physnet-trunk": "br-trunk"
        }
----

===== Creating Provider Networks In Neutron

Now the provider network(s) must be mapped to Neutron networks so that they can be assigned to VM instances.

To create a Neutron network for a VLAN interface on VLAN 201:

[subs=+quotes]
----
 # *neutron net-create --provider:physical_network physnet-trunk \
--provider:network_type vlan --provider:segmentation_id 201 \
--shared vlan201_network*
----

This creates a Neutron network named “vlan201_network” that maps to the physical network physnet-trunk using VLAN 201.

To create a Neutron network for a flat interface:

[subs=+quotes]
----
 # *neutron net-create --provider:physical_network physnet-trunk \
--provider:network_type flat --shared flat_provider_network*
----

This creates a Neutron network named _flat_provider_network_ that maps
to the physical network bridge _physnet-trunk_ but uses no VLAN tagging.

===== Associate a Subnet With A Provider Network

Finally, a subnet must be assigned to the provider network. This can be done for the VLAN interface in the example above using this command:

[subs=+quotes]
----
 # *neutron subnet-create vlan201_network 192.168.0.0/24*
----

==== Launching VMs

The VMs used for testing in this reference architecture were Fedora 20 x86_64 running inside m1.small KVM profiles. The Compute hosts were idle except for the test VM images, and there was no oversubscription of memory or CPU resources. VMs were launched from Horizon and used the default security group, with the addition of allowing incoming SSH.

When launching a VM, Neutron networks can be assigned to virtual NICs
on the VM. Typically the network attached to NIC 1 provides DHCP services for the VM. The network controllers should be running a neutron-dhcp-agent process for the network, or there should be infrastructure DHCP services on that network.

[[image-net-horizon]]
.image-net-horizon
image::images/ra_ospnet_8.png[caption="Figure 5.1: " title="Selecting Networks for VM in Horizon" align="center"]

==== Floating IPs

The Floating IP functionality and operation of Neutron is significantly different from Nova Networking. In Neutron, Floating IPs are attached to a Neutron router. Neutron routers are implemented using a neutron-l3-agent process running on the Neutron controller(s). The L3 agent uses iptables to implement floating IPs to do the network address translation (NAT). The agent also performs source NAT on outbound traffic that is destined for addresses outside the cloud. Filtering is performed according to rules in the applicable Nova Security Group that is applied to the VM.

In order for Floating IPs to function correctly, a Neutron router must
have interfaces on two networks: the Tenant network where the VMs are
attached, and an External network that has external reachability. If
the Floating IPs are to be accessible from the Internet, public IP
addresses must be used on the External network and a public IP must be
assigned to the Neutron router. In Figure 5.2 the Tenant1_router is
attached to both the _External_ network and the _Tenant_External_
network, and provides Floating IPs in the 10.1.247.64/27 range for the pictured VMs.

[[image-ra-net]]
.image-ra-net
image::images/ra_ospnet_9.png[caption="Figure 5.2: " title="Reference Architecture Neutron Network Topology" align="center"]

