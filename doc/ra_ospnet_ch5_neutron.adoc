[chapter 5]
== Implementing OpenStack Networking (Neutron)

=== Configure Neutron Networks

Before launching any virtual machines, it is necessary to configure the virtual
infrastructure, including networks that the VMs use. This section describes the
configuration of Neutron networks for the OpenStack deployment.

==== Create Tenant Networks and Subnets

Tenant networks can be created using Horizon. If no gateway is specified, the
first IP address on the network is used as the gateway.

Here are the networks that were created for this reference architecture:

.Reference Architecture Networks
[options="header"]
|====
|Network Name|Network Subnet|Gateway
|default-net|172.21.0.0/24|172.21.0.1
|ext-net|10.8.148.0/24|10.8.148.254
|====

External networks must be created to enable access outside the cloud. This is
usually done on the command line of the Undercloud using the Overcloud credentials
(run "source overcloudrc"). The Neutron router on the external network is connected
to both the Public network and the External network(s), and performs many-to-one
source NAT for VMs without Floating IPs as well as 1:1 NAT for VMs with Floating
IPs. Horizon does not support creating external networks, so they can be created
on the command-line with this command (assuming the floating IPs are on the bridge
mapped to "datacentre"):

[subs=+quotes]
----
 $ *neutron net-create ext-net --router:external \
 --provider:physical_network datacentre \
 --provider:network_type vlan \
 --provider:segmentation_id 104*
----

NOTE: It is recommended that the Floating IP networks be implemented
as VLANs as in the above command, because this allows for multiple
Floating IP networks and/or multiple Provider External networks. If,
however, the native VLAN is used on a link to provide floating IPs,
then the following command creates the floating IP network directly on the
bridge with no VLAN ID:

[subs=+quotes]
----
 $ *neutron net-create ext-net --router:external \
 --provider:physical_network datacentre \
 --provider:network_type flat*
----

Next a subnet must be created for the network, and a range of IPs
for Floating IPs are  assigned:

[subs=+quotes]
----
 $ *neutron subnet-create --name ext-subnet \
 --enable_dhcp=False \
 --allocation-pool start=10.8.148.50,end=10.8.148.100 \
 --gateway 10.8.148.254 \
 ext-net 10.8.148.0/24*
----

A subnet can be added to the network in Horizon by selecting *More* >
*Add Subnet* for the network in the *Networks* tab. The external
gateway must be specified. In the *Subnet Detail*, *Enable DHCP* must be disabled for External networks.

Next we create a default network for tenant VMs. This network is
internal to the Overcloud, and external access is through a floating IP.

[subs=+quotes]
----
 $ *neutron net-create default-net –provider:vxlan*
 $ *neutron subnet-create --name default-subnet default-net
 172.21.0.0/22*
----

Nova instances can then be created and attached to the default-net
network. Optionally, Floating IP addresses may be associated with the
instance for external connectivity.

==== Gateways and L3 Agents

Neutron routers perform the task of forwarding traffic between
networks. A router can be attached to multiple tenant networks, and
routes traffic between them. A router can also be attached to one or
more external networks, permitting outbound SNAT for VMs, or allowing
floating IPs on the external network to be assigned to VMs on the
tenant networks that the router is attached to. When interfaces are
created on the router, ports are attached to the _neutron-l3-agent_
process on the Network Controller(s).

When a router is created in Neutron, initially it is not connected to
anynetworks. The router needs to be attached to the networks by
creating interfaces, either in Horizon or on the command line.

Steps for creating routers to route traffic between Neutron networks:

1. Create the router in Horizon, giving the router a name in the
   dialog box, or run the following command:
+
[subs=+quotes]
----
 # *neutron router-create router-1to2*
 Created a new router:
 +-----------------------+--------------------------------------+
 | Field                 | Value                                |
 +-----------------------+--------------------------------------+
 | admin_state_up        | True                                 |
 | external_gateway_info |                                      |
 | id                    | 63285650-16b9-46e5-9a55-34701c972605 |
 | name                  | router-1to2                          |
 | status                | ACTIVE                               |
 | tenant_id             | eda22eb1991d4af285cbf60bb6847b84     |
 +-----------------------+--------------------------------------+
----
+
2. Once the router is created, attach interfaces to the router in Horizon, or run the following command, using the name of the router and the name or ID of the subnet to attach to the router.
+
[subs=+quotes]
----
# *neutron router-interface-add router-1to2 subnet=subnet-1*
Added interface 7fa87edf-6504-4d50-9886-98951baa9c93 to router router-1to2.
# *neutron router-interface-add router-1to2 subnet=subnet-2*
Added interface fbe69fb7-d5b2-4ea6-835a-9ae8b6274853 to router router-1to2.
----

The above commands create a router named “router-1to2” between the subnets named “subnet-1” and “subnet-2” using the default router address (the first IP address on the subnet) on each subnet.

To use a different IP (other than the first IP in the network), first
create a port with the desired IP using the _neutron port-create_
command, then use the parameter port= instead of subnet= with the
_neutron router-interface-add_ command.

[[configure_mtu]]
==== Configure MTU

To configure the default tenant MTU settings to use jumbo frames (larger
than 1500 bytes), you can specify the parameter
_NeutronTenantMtu_ in an environment file. This will set the default MTU
 for tenant networks created by Neutron. If you are using any kind of
tunneling (VXLAN or GRE), then this value should be 50 bytes less than the
physical MTU set on the network switches.

[subs=+quotes]
----
parameter_defaults:
  NeutronTenantMtu: 8950
----

NOTE: This parameter is deprecated. Future versions of OpenStack Networking
will automatically determine this value. Additional parameters will be made
available for fine-tuning tenant MTU settings.

==== Configuring Provider Networks
Provider networks are networks that are created by the Operator rather than
by tenants. This is often used as a way to attach a VM directly to an
existing flat or VLAN network that is a part of the datacenter network.
This is often how external access is provided, rather than using floating
IPs on a Neutron controller. Provider networks and Neutron tenant networks
can both be used simultaneously by a VM.

Provider networks can only be configured by an administrator, but they appear
as normal Neutron networks to the client if they are marked as shared. This
allows them to be selected by a tenant when launching a VM.

Provider networks generally do not require the use of the L3 Agent, because the traffic is
not routed through the Neutron controller. Generally Neutron will provide DHCP
services on provider networks. Ordinarily, metadata services are provided by a
redirect on the L3 Agent, but another mechanism is provided below for metadata
services on provider networks.

===== Configuring Neutron For Provider Networks

Neutron maps provider networks to a bridge, and maps that bridge to a physical
adapter or VLAN interface. These mappings must be made in the
_ovs_neutron_plugin.ini_ file on the controllers and compute hosts:

[subs=+quotes]
----
 #/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini
 bridge_mappings = physnet-trunk:br-trunk
 network_vlan_ranges = physnet-trunk
----

This is done automatically by Director based on the mappings provided on the
deployment command-line. The default mapping is datacentre:br-ex. This results
in the "br-ex" bridge being referred to as the physical network "datacentre"
when creating networks. The above mapping would result in the bridge "br-trunk"
being referred to as the physical network "physnet-trunk". Multiple bridge
mappings may be separated by commas in the configuration file and in the
deployment command line.

===== Configuring the Metadata Service For Provider Networks

The normal mechanism of providing metadata services via a redirect on the L3 Agent is not compatible with provider networks. Instead, configure the file _/etc/neutron/dhcp_agent.ini_ as follows:

[subs=+quotes]
----
 #/etc/neutron/dhcp_agent.ini
 enable_isolated_metadata = True
 enable_metadata_network = True
----

NOTE: Utilizing this configuration will supersede the L3-provided metadata
services for networks that do not use provider networks, but this method
should also be compatible with those networks.

This setting can be added to the network environment like this:

[subs=+quotes]
----
parameter_defaults:
  NeutronEnableIsolatedMetadata: true
----


===== Creating Provider Network Bridge With OSP-Director

In order to use provider networks, the interface to the network will have to be
placed on a bridge. By default, a br-ex bridge will be created, but provider
networks can also be assigned to interfaces which are not part of br-ex by
creating another bridge. The bridge should match on both the controller and
compute nodes. For example, if bond2 (with nic5 and nic6) will be attached
to provider networks, then adding this to both the controller and compute
NIC configuration will allow provider networks to be created on this bond:

[subs=+quotes]
----
            -
              type: ovs_bridge
              name: br-trunk
              members:
                -
                  type: linux_bond
                  name: bond2
                  bonding_options: {get_param: BondInterfaceOvsOptions}
                  members:
                    -
                      type: interface
                      name: nic5
                      primary: true
                    -
                      type: interface
                      name: nic6
----

===== Creating Provider Network Bridge Manually
Typically the provider network bridge should be added to the NIC configuration
templates, so that the bridges will be created at deployment time.

If the bridges were not created at deployment time, then create the
bridges that were referenced in the _ovs_neutron_plugin.ini_ file on the
controllers and compute hosts. To add a VLAN interface that is trunked to the
host via bond2, add the bond2 interface:

[subs=+quotes]
----
 # ovs-vsctl add-br br-trunk
 # ovs-vsctl add-port br-trunk bond2
----

Alternately, to add a physical interface eth3 with a flat network:

[subs=+quotes]
----
 # ovs-vsctl add-br br-trunk
 # ovs-vsctl add-port br-trunk eth3
----

At this point the Neutron services will have to be restarted on all controllers
and compute hosts. If making the changes on an HA deployment, restart only one
controller at a time and wait for it to rejoin the cluster before restarting
the services on the next server.

===== Validating Bridge Mapping

Neutron should be aware of all the bridge mappings on all compute hosts. To
validate this, use the Neutron commands to show each compute host:

[subs=+quotes]
----
 # *neutron agent-list*
 # *neutron agent-show <uuid>*
----

You should see this in the data returned by the _agent-show_ command:

[subs=+quotes]
----
    "bridge_mappings": {
                "physnet-trunk": "br-trunk"
        }
----

===== Creating Provider Networks In Neutron

Now the provider network(s) must be mapped to Neutron networks so that they
can be assigned to VM instances.

To create a Neutron network for a VLAN interface on VLAN 201:

[subs=+quotes]
----
 # *neutron net-create --provider:physical_network physnet-trunk \
--provider:network_type vlan --provider:segmentation_id 201 \
--shared vlan201_network*
----

This creates a Neutron network named “vlan201_network” that maps to the physical
network physnet-trunk using VLAN 201.

To create a Neutron network for a flat interface:

[subs=+quotes]
----
 # *neutron net-create --provider:physical_network physnet-trunk \
--provider:network_type flat --shared flat_provider_network*
----

This creates a Neutron network named _flat_provider_network_ that maps
to the physical network bridge _physnet-trunk_ but uses no VLAN tagging.

===== Associate a Subnet With A Provider Network

Finally, a subnet must be assigned to the provider network. This can be done for
the VLAN interface in the example above using this command:

[subs=+quotes]
----
 # *neutron subnet-create vlan201_network 192.168.0.0/24*
----

==== Launching VMs

The VMs used for testing in this reference architecture were Fedora x86_64
running inside m1.small KVM profiles. The Compute hosts were idle except for
the test VM images, and there was no oversubscription of memory or CPU
resources. VMs were launched from Horizon and used the default security group,
with the addition of allowing incoming SSH.

When launching a VM, Neutron networks can be assigned to virtual NICs
on the VM. Typically the network attached to NIC 1 provides DHCP
services for the VM. The network controllers should be running a
_neutron-dhcp-agent_ process for the network, or there should be
infrastructure DHCP services on that network.

[[image-net-horizon]]
.image-net-horizon
image::images/ra_ospnet_8.png[caption="Figure 5.1: " title="Selecting Networks
for VM in Horizon" align="center"]

==== Floating IPs

The Floating IP functionality and operation of Neutron is
significantly different from Nova Networking. In Neutron, Floating IPs
are attached to a Neutron router. Neutron routers are implemented
using a _neutron-l3-agent_ process running on the Neutron
controller(s). The L3 agent uses _iptables_ to implement floating IPs
to do the network address translation (NAT). The agent also performs
source NAT on outbound traffic that is destined for addresses outside
the cloud. Filtering is performed according to rules in the applicable
Nova Security Group that is applied to the VM.

In order for Floating IPs to function correctly, a Neutron router must
have interfaces on two networks: the Tenant network where the VMs are
attached, and an External network that has external reachability. If
the Floating IPs are to be accessible from the Internet, public IP
addresses must be used on the External network and a public IP must be
assigned to the Neutron router. In Figure 5.2 the Tenant1_router is
attached to both the _External_ network and the _Tenant_External_
network, and provides Floating IPs in the 10.1.247.64/27 range for the pictured
VMs.

[[image-ra-net]]
.image-ra-net
image::images/ra_ospnet_9.png[caption="Figure 5.2: " title="Reference Architecture Neutron Network Topology" align="center"]

