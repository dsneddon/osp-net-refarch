[chapter 2]
== Architecture Overview

This section describes the software and hardware components used to
develop this reference architecture.

=== OpenStack Platform

{osp} provides the foundation to build private or public
Infrastructure-as-a-Service (IaaS) for
cloud-enabled workloads. It allows organizations to leverage
OpenStack, the largest and fastest growing open source cloud
infrastructure project, while maintaining the security, stability, and
enterprise readiness of a platform built on {rhel}.
{osp} gives organizations a
truly open framework for hosting cloud workloads, delivered by Red Hat
subscription for maximum flexibility and cost effectiveness. In
conjunction with other Red Hat technologies, {osp}
allows organizations to move from traditional
workloads to cloud-enabled workloads on their own terms and timelines,
as their applications require. Red Hat frees organizations from
proprietary lock-in, and allows them to move to open technologies
while maintaining their existing infrastructure investments.

Unlike other OpenStack distributions, {osp} provides a certified ecosystem of hardware,
software, and services, an enterprise lifecycle that extends the
community OpenStack release cycle, and award-winning Red Hat support
on both the OpenStack modules and their underlying Linux dependencies.
Red Hat delivers long-term commitment and value from a proven
enterprise software partner so organizations can take advantage of the
fast pace of OpenStack development without risking the stability and
supportability of their production environments.

==== {ospver}
{ospver} ({ro} 7) is based on
the upstream “Kilo” OpenStack release. {ospver}
is Red Hat’s seventh release of the OpenStack
Platform. The first release was based on the “Essex” OpenStack
release. {ro} 7 contains many new features delivered with Juno, as
well as the additional hardening, stability, and functionality by
validating upstream bugs, as well as integration with other Red Hat
products, and comprehensive documentation.

Minimum requirements for server hardware for {osp}:

Controller nodes

* 64-bit x86 processor with support for the Intel 64 or AMD64 CPU
extensions.
* A minimum of 4 GB of RAM is recommended.
* A minimum of 40 GB of available disk space is recommended, and more
for nodes providing block storage services. See documentation for more
details.
* 2 x 1 Gbps (or 2 x 10 Gbps) network interface cards.
* Intelligent Platform Management Interface (IPMI) functionality on
motherboard.

Compute nodes

* 64-bit x86 processor with support for the Intel 64 or AMD64 CPU
extensions, and the AMD-V or Intel VT hardware virtualization
extensions enabled.
* A minimum of 2 GB of RAM is recommended.
* A minimum of 40 GB of available disk space is recommended.
* 2 x 1 Gbps (or 2 x 10 Gbps) network interface cards.
* Intelligent Platform Management Interface (IPMI) functionality on
motherboard.

Ceph nodes

* 64-bit x86 processor with support for the Intel 64 or AMD64 CPU
extensions, and the AMD-V or Intel VT hardware virtualization
extensions enabled.
* A minimum of 1 GB of RAM per 1 TB of hard disk storage space is
recommended.
* Storage space depends on memory. Ideally, use at minimum 1 GB of
memory per 1 TB of storage.
* 2 x 1 Gbps (or 2 x 10 Gbps) network interface cards.
* Intelligent Platform Management Interface (IPMI) functionality on
motherboard.

=== Network Architecture

This section introduces the basic network architecture of {ro} 7.
The installation is now performed using OpenStack on OpenStack (OoO or
“TripleO”) to deploy the production cloud.  The installation server
(or servers) is running a self-contained instance of OpenStack
referred to as the Undercloud. The OpenStack services on the
Undercloud are utilized to deploy an instance of OpenStack across a
set of bare metal servers. We call the deployed instance the
Overcloud.

==== Networking Layers

There are several layers to the networking in the overall deployment:
Undercloud provisioning, Overcloud operation, and virtual machine
connectivity.

===== Undercloud Networking

The Undercloud utilizes the Provisioning network for DHCP and PXE to
deploy the Overcloud nodes. The instance of Heat running on the
Undercloud also uses the Provisioning network for orchestrating the
Overcloud. The Undercloud is attached to this network with the
interface that is running PXE services. Generally the first network
interface is used on the Overcloud nodes, since the system will PXE
boot from the first interface by default.

The Undercloud also requires an uplink with Internet access that is
separate from the interface used to serve DHCP and PXE services. It
may share the External VLAN used for the Horizon dashboard and Public
APIs, which will be how the Undercloud contacts the Overcloud APIs.

===== Overcloud Networks

The Overcloud networks are connected to the servers that are part of
the Overcloud. They provides: control traffic between the HA
controllers and the compute and storage nodes; connectivity for tenant
VM traffic; object and block storage traffic; and external
connectivity with floating IPs. In the reference architecture each of
these traffic types uses a separate VLAN for traffic isolation.

===== Tenant Networks

The virtual networks that are created for tenant VMs and managed by
Neutron. The tenant networks are provided via an abstraction layer
that separates traffic by tenant and network using either reserved
VLAN ranges or IP tunneling using VXLAN or GRE.

Virtual tenant networking is sometimes referred to as Overlay
networking, because the tenant networks are implemented as a virtual
network on top of the physical network. Overlay networks are
implemented in bridges on the Compute hosts (either provided by OVS or
Linux Bridges). From the VM perspective, the Overlay network appears
to be a physical network connecting the VMs.

[[image-physical-network]]
.image-physical-network
image::images/ra_ospnet_1.png[caption="Figure 2.1: " title="Controllers and Hosts Connected Via Physical Network" align="center"]

The Controllers and Compute nodes are connected to one another via the
Tenant network. The tenant networks are managed by Neutron using
either VLAN tags or tunneling using VXLAN or GRE. Traffic within a
tenant network is isolated to that tenant, and to the virtual machines
attached to that particular tenant network.

[[image-tenant-overlay]]
.image-tenant-overlay
image::images/ra_ospnet_2.png[caption="Figure 2.2: " title="Tenant Networks Running in Overlay" align="center"]

==== Minimum Network For Provisioning

An Undercloud server deploys OpenStack onto bare metal servers by
using DHCP/PXE in order to initiate a TFTP boot of an image. In order
to assign IP addresses and provide a TFTP address, the bare metal
servers must be connected to the Provisioning network.

NOTE: In a standard deployment, the Undercloud server runs DHCP and
TFTP services in order to PXE boot the Overcloud nodes. The TFTP
protocol has no concept of security, and the server may be a security
risk if left exposed. This network should be isolated or otherwise
secured in production deployments.

When servers are provisioned, Puppet will run to configure the
OpenStack services. All nodes must be connected to the Provisioning
network via the network interface from which they boot. This is
usually the first network interface, often referred to as eth0, but it
may be configured via the system's BIOS to be a different interface.
The installation server must be attached to this network via one of
its interfaces.

==== Isolated Networks For OpenStack Deployment

For production deployments, there are advantages to dividing the
Overcloud into several network segments. This allows the network
traffic to be distributed between interfaces or bonds. It also
prevents mixing control traffic and data traffic in the same network.

[glossary]
*Provisioning Network*::
  This network is required when using a
  provisioning server, or an installation server that performs PXE boot
  and installation of the operating system and OpenStack components.
  This network is also used by Heat on the Undercloud to perform
  orchestration of the Overcloud. In a minimal or proof-of-concept
  deployment, it is possible to run all traffic over a single flat
  network, but this mode is not recommended for production. The
  Provisioning network will be used by the Compute and Storage nodes to
  access NTP, DNS, and system updates. The Undercloud may be used as a
  default gateway, but a router gateway on the Provisioning network is
  recommended for resilience and scalability (so the traffic is not
  routed through the Undercloud).
*Internal API Network*::
  This network is used for connections to the API
  servers, as well as RPC messages using RabbitMQ and connections to the
  database. The Glance Registry API uses this network, as does the
  Cinder API. This network is typically only reachable from inside the
  OpenStack Overcloud environment, so API calls from outside the cloud
  will use the Public APIs.
*Tenant Network*::
  This network is used for connectivity for VMs. A
  single network VLAN is used when Neutron is using VXLAN or GRE
  tunneling mode. When using VLAN mode, however, a range of VLANs must
  be set aside for tenant networks. These VLANs should not be in use for
  any other purpose. Neutron will assign the VLANs to a VM based on
  which tenant networks are assigned to the VM.
*Storage Network*::
  This network hosts the data traffic for object and
  block storage. Cinder iSCSI connections are made on the Storage
  network. The Swift API and Glance API utilize this network, since
  those APIs transfer storage data. It is recommended that that the
  interface used on the Storage network be 10Gb Ethernet or faster if
  possible. If this network is attached to a bond shared with other
  networks, there must be sufficient bandwidth available. It is common
  to use higher bandwidth links, or a bond with larger capacity, at the
  storage controller than is used for each compute host. For instance,
  if the compute hosts are connected to the storage network over a 10 Gb
  connection, the storage controller may be connected over a 40 Gb bond
  using 4 x 10 Gb links.
*Storage Management Network*::
  This network is used for back-end storage
  operations. Swift uses this network to perform replication between
  storage nodes, and the Swift Proxy on the controllers will use this
  network to access the raw storage on dedicated Swift storage nodes.
  Ceph uses this network for clustering.
*External Network*::
  The External network is used for hosting the
  Horizon dashboard and the Public APIs, as well as hosting the floating
  IPs that are assigned to VMs. The Neutron L3 routers which perform NAT
  are attached to this interface. The range of IPs that are assigned to
  floating IPs should not include the IPs used for hosts and VIPs on
  this network.

==== Network VLAN Requirements
The networks used are on separate VLANs. In a typical OpenStack
installation, the number of networks will exceed the number of
physical network links, especially when using Ethernet bonds. In order
to connect all the networks to the proper hosts, VLAN tagging is used
to deliver more than one network per interface. Most of the networks
are isolated subnets, but some will require a Layer 3 gateway to
provide routing for Internet or infrastructure network connectivity.
Some networks can be delivered over tagged VLANs, but certain network
types have restrictions or must be delivered over dedicated
interfaces.

When using HA controllers, over 30 addresses are used for HA virtual
IPs (VIPs). These addresses are spread between the Cluster Management,
Internal API, and Public API subnets. If any of these subnets are
combined, a minimum subnet size of /26 (62 usable IP addresses) is
required for the resulting subnet.

Internet access (or access to external networks) is required for the
Tenant External networks in order to provide external connectivity for
VMs.

.Network VLAN requirements
[options="header"]
|====
|Network|Native VLAN|Internet/External|Notes
|Provisioning (DHCP + PXE)|XXX||First Interface Recommended
|Internal API|||
|Tenant Networking|||
|Storage|||
|Storage Management|||
|External||XXX|
|Installer Uplink||XXX|May use existing infrastructure for uplink
|====

==== Network Types By Node Type

[[image-network-type]]
.image-network-type
image::images/ra_ospnet_3.png[caption="Figure 2.3: " title="RHEL-OSP Networks by Node Type" align="center"]

The following network traffic types are used in {osp}:

.Network types by node type
[options="header"]
|====
||{ro} Installer|Compute Nodes|HA Controllers|Storage Nodes
|Provisioning (DHCP + PXE)|XXX|XXX|XXX|XXX
|Internal API||XXX|XXX|
|Tenant||XXX|XXX|
|Storage||XXX|XXX|XXX
|Storage Management|||XXX|
|External|||XXX|
|Installer External Uplink|XXX||
|====

==== Example Physical Network Interface Assignment
Here is an example of physical interface assignment. There are two
rules that must be followed:

1. Provisioning must be on a dedicated interface, or it must be the
   native VLAN on a trunked interface.
2. The physical interface chosen for the Provisioning network must be
   enabled for PXE boot via DHCP. The first interface is usually
   enabled by default.

Undercloud:

* eth0: Uplink with Internet connectivity
* eth1: Provisioning (DHCP server, TFTP server, iPXE HTTP server)

Overcloud:

* eth0: Provisioning
* eth1: unused
* eth2 + eth3: Bond carrying the following VLANs:
** Internal API
** Tenant
** Storage
** Storage Management
** External

==== Neutron Networking Modes Overview

Neutron supports a variety of networking modes using plugins. The
modes covered in this document are supported by the Neutron
Modular Layer 2 (ML2) plugin, which has a variety of drivers to
support different networking models. At a high level, these
modes are VLAN, GRE, and VXLAN, which are described in detail in
section 2.3.3 (“Neutron ML2 Plugin Type Drivers”).

==== High Availability Network Methodologies Overview

Some high availability methodologies can be added to an existing
deployment, but some need to be designed into the network from
the beginning. High availability methodologies are covered
further in Section 3.

These methodologies are infrastructure-oriented, and must be
included in the initial design:

* Leaf-and-spine network topology
* Multi-path network topology
* Ethernet bonding
* Multi-chassis link aggregation (also known as “switch stacking”
or “switch clustering”)
* Redundant network hardware
* Redundant power supplies (ideally fed from multiple sources)

These high-availability services may be installed at deployment time,
or added to an existing deployment:

* High-availability OpenStack components
* Database clustering
* Load balancing

==== Network Architectures Not Described in this Document

This section describes the Neutron networking models which may
be added to {ro} but are not supported because they are
enabled through third-party Provider plug-ins. Red Hat has
partnerships with the leading vendors in the networking
ecosystem. A full list of the certified plugins and drivers is
available at https://access.redhat.com/certifications.

===== Third-Party Provider Networks

Neutron supports functionality through plugins, and a variety of
hardware and software vendors supply plugins to support
networking models that require their hardware or software.
Cisco, Arista, Big Switch, PLUMGrid, Mellanox, VMWare, Nuage,
and others provide and support these plugins. As such, these
plugins are not covered in this document.

===== Software Defined Networking

A variety of Software Defined Networking (SDN) solutions exist
for OpenStack, and most are enabled with a plugin to Neutron
that offloads the management and operation of the network
functions to software controllers, or a combination of software
and hardware. Some of these solutions are open-source, such as
Open Contrail, MidoNet, or OpenDaylight. Others are proprietary
commercial solutions, such as VMware NSX, or PLUMGrid. Using SDN
significantly changes the desired network architecture, and
introduces one or more controllers to manage overlay networks.
SDN solutions have their own installation and operation
methodologies, which are not covered in this document.

=== OpenStack Networking With Neutron

This section describes OpenStack Networking. Networking in
OpenStack is provided in two ways. The older method, known as
Nova Networking, supports several modes based on a shared
network infrastructure. The shared infrastructure may be flat, a
set of shared VLANs, or some combination. OpenStack Networking
is a standalone service centered around the Neutron server.
Neutron provides network virtualization, and is extensible
through the use of plug-ins.

====  Neutron Server

This service runs on the Network nodes (non HA), or the
Controller nodes (HA) to service the Network Service API and its
extensions. It enforces the network model and IP addressing of
each port. The neutron-server and plugin agents require access
to a database for persistent storage and access to a message
queue for inter-communication.

====  Neutron Plugin Architecture

Most of the functionality of Neutron is provided with the
selected plugin. Each plugin has a component that runs on the
Network nodes (non HA) or Controller nodes (HA), and some
plugins have a plugin-agent that runs on each compute node. The
plugin that runs on each compute node manages the local virtual
switch (vswitch) configuration. The plug-in that you use with
Neutron server determines which plugin agents run on each
compute node. For example, a common configuration is the ML2
plugin plus the neutron_ovs_agent for L2 communication on the
Compute host.

===== Neutron Plugins Included in main Neutron Distribution

Several plugins are included in the Neutron distribution, and
are included in the OpenStack Neutron packages for RHEL OSP.
Additionally, vendor-specific plugins and drivers are provided
by manufacturers of certain network hardware. A full list of the
certified plugins and drivers is available at
https://access.redhat.com/certifications.

[glossary]
*Core Plugins*::
  These plugins provide the core functionality for
  Neutron. In this reference architecture the ML2 core plugin is
  used. The ML2 plugin contains type and mechanism drivers:
 - *Type Drivers*: These drivers are managed by the core plugin, and
  provide support for various network architectures. The types
  supported by ML2 include local, flat, VLAN, GRE, and VXLAN. In
  this reference architecture the GRE tunnel mechanism is used.
 - *Mechanism Drivers*: These drivers are responsible for taking the
  information established by the type driver and ensuring that it
  is properly applied given the specific networking mechanisms
  that have been enabled. Mechanism drivers include the Linux
  Bridge, Open vSwitch, and vendor-specific mechanisms for certain
  hardware vendors. In this reference architecture the Open
  vSwitch mechanism is used.
*Service Plugins*::
  These plugins provide additional services,
  such as external gateway connectivity, firewall, load balancer,
  and other services. In this reference architecture the L3 Router
  service plugin is used.

===== Additional Neutron Plugins From Other Sources

Although this reference architecture does not cover the
vendor-specific plugins provided by other sources, there are a
growing number of plugins available from hardware and software
vendors. In general, the vendor-specific core plugins include
type drivers and mechanism drivers which correspond to a
particular brand of network equipment or to software that
provides software defined networking, or both. In some cases,
such as with Cisco networking hardware, a particular brand of
network hardware can either be used with the vendor-specific
plugin or with the included Neutron ML2 plugin, depending on
whether vendor-specific functionality is desired.  A full list
of the certified plugins and drivers is available at
https://access.redhat.com/certifications.

====  Neutron ML2 Plugin Type Drivers

This section describes the type drivers that are available in
the Neutron ML2 plugin. These type drivers correspond to the
network architecture which has been selected for the Overcloud.

===== Flat

The flat type driver is used when all nodes will share a single
or multiple flat networks, with no VLAN separation or tunnels
used to abstract the network or provide network separation. This
type of network is most commonly used when an OpenStack
deployment is used for a private cloud that is being used for a
single purpose or application. It does not provide tenant
separation or isolation, and all virtual machines share IP
address ranges.

===== VLAN

The VLAN type driver is used when the separation between tenants
or projects is enforced within the network infrastructure and
managed within Neutron. The network switches are configured with
a range of VLANs, and the VLANs are trunked on the physical
connections to the hosts. Neutron is configured with the virtual
networks mapped to VLANs. When VMs are launched, they are
attached to the appropriate VLANs according to their
configuration within Nova. VLANs provide isolation and
segregation, and Neutron uses Linux Network Namespaces to allow
overlapping IP address ranges between VLANs. For instance, two
or more virtual networks could use the 192.168.0.0/24 subnet.
Within the range of VLANs assigned to Neutron, all management of
the VLANs is handled by Neutron. For this reason, the entire
range of VLANs must be trunked to each compute host. Neutron
will instruct the mechanism driver on the Compute host to
automatically attach the appropriate VLANs to the virtual ports
assigned to each VM on the host.

===== GRE

The GRE (Generic Routing Encapsulation) type driver provides an
alternative form of network segregation. Virtual networks are
created within Neutron, and then associated with VMs in Nova.
When VMs are launched, the mechanism driver attaches the virtual
network interface of the VM to a virtual network. The mechanism
driver on the Compute host forms GRE tunnels between the virtual
network interface of the VM and the virtual network interface of
all other VMs on the same virtual network. This mesh is used
both for point-to-point traffic between one VM and another, and
broadcast traffic, which is sent simultaneously to all VMs on
the virtual network.

GRE tunnels provide network segmentation and isolation, and
Neutron uses Linux Network Namespaces to allow overlapping IP
address ranges between Tenants. For instance, two or more
virtual networks could use the 192.168.0.0/24 subnet. Virtual
networks are per-tenant, so only VMs belonging to the same
tenant can share a given virtual network. VMs in different GRE
tunnel overlay networks cannot talk to each other unless virtual
routers are set up to route between networks.

GRE tunnels are transparent to the VM. From the VM's
perspective, it is sharing a layer 2 segment with the other VMs
on the virtual network. GRE endpoints on the Compute host
encapsulate and decapsulate traffic, so the traffic sent and
received by VMs is unaffected by the GRE tunnel.

NOTE: Red Hat recommends the OVS mechanism driver in conjunction with
the VXLAN type driver. It is also possible to use the Linux
Bridge in place of OVS for connectivity inside the hypervisor.
This may be preferable in certain circumstances, such as when
using SR-IOV.

===== VXLAN

The VXLAN (Virtual Extensible LAN) type driver provides another
method of network segregation. Virtual networks are created
within Neutron, and then associated with VMs in Nova. When VMs
are launched, the mechanism driver attaches the virtual network
interface of the VM to a virtual network. The VXLAN mechanism
driver on each compute host encapsulates each layer 2 Ethernet
frame sent by the VMs in a layer 4 UDP packet. The UDP packet
includes an 8-byte field, within which a 24-bit value is used
for the VXLAN Segment ID. The VXLAN Segment ID is used to
designate the individual VXLAN over network on which the
communicating VMs are situated.

VXLAN Segment IDs provides network segmentation and isolation,
and Neutron uses Linux Network Namespaces to allow overlapping
IP address ranges between the virtual networks. For instance,
two or more virtual networks could use the 192.168.0.0/24
subnet. Virtual networks are per-tenant, so only VMs belonging
to the same tenant can share a given virtual network. VMs in
different VXLAN overlay networks cannot communicate with each
other unless virtual routers are set up to route between
networks.

VXLAN encapsulation is transparent to the VM.  From the VM's
perspective, it is sharing a layer 2 segment with the other VMs
on the virtual network. VXLAN endpoints encapsulate and
decapsulate traffic, so the traffic sent and received by VMs is
unaffected by the VXLAN encapsulation.
Red Hat recommends the OVS mechanism driver in conjunction with
the VXLAN type driver.  It is also possible to use Linux Bridge
in the place of OVS for connectivity inside the hypervisor. This
may be preferable in certain circumstances, such as when using
SR-IOV.

==== Neutron L3 Agent

The Neutron L3 Agent acts as a layer 3 router for tenant
networks. Without a router, VMs in a tenant network are only
able to communicate with one another. Creating a router and
assigning it to a tenant network allows the VMs in that network
to communicate with other tenant networks (also known as
East-West routing) or upstream if an external gateway is defined
for the router (also known as North-South routing).
When a router is created in Neutron, it is assigned to a Neutron
network. The router is not started until the first VM in that
network is created.

=== Neutron High Availability

Pacemaker is used to create a cluster of 3 or more controllers.
Each cluster will support the same bridges, tenant networks, and
Neutron agents. It is possible to configure multiple HA clusters
for load sharing. Each cluster would maintain a different set of
networks and agents.

Making Neutron highly available requires a variety of approaches
for the various agents:

- The *neutron-dhcp-agent* uses dnsmasq to provide DHCP services for
each tenant network. The DHCP protocol functions in a way that
allows multiple DHCP servers per network.
- The *L3 Agent* uses iptables to implement NAT for the tenant
networks. External clustering can create an active/standby
controller pair, and IP addresses are failed over. When a
failover occurs, the existing TCP connections will be reset.
- The *Metadata agent* can be run on all network controllers, making
it highly available and scalable with multiple HA network
controllers.

==== Open vSwitch Bridges

The OVS bridges are created when the Neutron software is
installed and configured on each Network controller. When
Neutron routers are created, they will be attached to the
bridges using OVS tap interfaces.

==== L3 Agent High Availability

L3 agents handle persistent TCP streams, so it isn't possible to
run multiple L3 agents in the same way as running multiple DHCP
agents. L3 agents running on multiple controllers will have the
same networks and routers, but traffic is routed through a
virtual IP address which the controllers share. If one
controller fails, another will take over the virtual IP.
